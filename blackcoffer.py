# -*- coding: utf-8 -*-
"""Blackcoffer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZnUIbeTZ9X5Q2fJ4Yq0bAPHAQxO7qj8
"""

pip install pandas beautifulsoup4 requests openpyxl

pip install pandas nltk textstat

import pandas as pd
import requests
from bs4 import BeautifulSoup
import os
import nltk
import textstat
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re

# Ensure the NLTK stopwords are downloaded
nltk.download('stopwords')
nltk.download('punkt')

# Load additional stop words from the StopWords folder
stopwords_dir = '/content/drive/MyDrive/Black/StopWords'
stop_words = set(stopwords.words('english'))

for filename in os.listdir(stopwords_dir):
    if filename.endswith('.txt'):
        try:
            with open(os.path.join(stopwords_dir, filename), 'r', encoding='utf-8') as file:
                stop_words.update(file.read().split())
        except UnicodeDecodeError:
            with open(os.path.join(stopwords_dir, filename), 'r', encoding='latin1') as file:
                stop_words.update(file.read().split())

# Read URLs from Excel file
input_file = '/content/drive/MyDrive/Black/Input.xlsx'
df = pd.read_excel(input_file)

# Function to extract article title and text
def extract_article(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    title = soup.find('h1').get_text() if soup.find('h1') else 'No Title Found'
    paragraphs = soup.find_all('p')
    article_text = '\n'.join([p.get_text() for p in paragraphs])

    return title, article_text

# Function to clean and tokenize text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

# Function to count syllables in a word
def count_syllables(word):
    word = word.lower()
    count = len(re.findall(r'[aeiouy]+', word))
    if word.endswith('es') or word.endswith('ed'):
        count -= 1
    return max(1, count)

# Read positive and negative words from dictionaries
with open('/content/drive/MyDrive/Black/MasterDictionary/positive-words.txt', 'r') as file:
    positive_words = set(file.read().split())
with open('/content/drive/MyDrive/Black/MasterDictionary/negative-words.txt', 'r',encoding = 'latin1') as file:
    negative_words = set(file.read().split())

# Remove stop words from positive and negative words lists
positive_words = positive_words - stop_words
negative_words = negative_words - stop_words

# Function to perform sentiment analysis
def sentiment_analysis(tokens):
    positive_score = sum(1 for word in tokens if word in positive_words)
    negative_score = sum(1 for word in tokens if word in negative_words)
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)
    return positive_score, negative_score, polarity_score, subjectivity_score

# Function to calculate readability, personal pronouns, and other metrics
def text_analysis(article_text):
    tokens = clean_text(article_text)
    sentences = nltk.sent_tokenize(article_text)

    word_count = len(tokens)
    syllable_count_per_word = sum(count_syllables(word) for word in tokens) / word_count
    complex_word_count = sum(1 for word in tokens if count_syllables(word) > 2)
    average_word_length = sum(len(word) for word in tokens) / word_count

    positive_score, negative_score, polarity_score, subjectivity_score = sentiment_analysis(tokens)

    personal_pronouns = re.findall(r'\b(I|we|my|ours|us)\b', article_text, re.I)
    personal_pronouns_count = len(personal_pronouns)

    avg_sentence_length = word_count / len(sentences)
    percentage_complex_words = complex_word_count / word_count
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    return {
        'Positive Score': positive_score,
        'Negative Score': negative_score,
        'Polarity Score': polarity_score,
        'Subjectivity Score': subjectivity_score,
        'Average Sentence Length': avg_sentence_length,
        'Percentage of Complex Words': percentage_complex_words,
        'Fog Index': fog_index,
        'Word Count': word_count,
        'Complex Word Count': complex_word_count,
        'Syllable Count Per Word': syllable_count_per_word,
        'Personal Pronouns': personal_pronouns_count,
        'Average Word Length': average_word_length
    }

# Directory to save articles
if not os.path.exists('articles'):
    os.makedirs('articles')

# Directory to save the analysis results
if not os.path.exists('analysis'):
    os.makedirs('analysis')

# List to store the analysis results
analysis_results = []

# Loop through each URL in the dataframe
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    try:
        title, article_text = extract_article(url)

        # Save the article text to a file
        with open(f'articles/{url_id}.txt', 'w', encoding='utf-8') as file:
            file.write(f"Title: {title}\n\n{article_text}")

        # Perform text analysis
        analysis_result = text_analysis(article_text)
        analysis_result['URL_ID'] = url_id
        analysis_results.append(analysis_result)

        print(f'Successfully processed article {url_id}')

    except Exception as e:
        print(f'Failed to extract or process article {url_id}: {e}')

# Convert analysis results to a DataFrame
analysis_df = pd.DataFrame(analysis_results)

# Read existing output structure
output_df = pd.read_excel('/content/drive/MyDrive/Black/Output Data Structure.xlsx')

# Drop rows where URL_ID does not exist (404 error handling)
output_df = output_df[output_df['URL_ID'].isin(df['URL_ID'])]

# Define computed variables
variables = [
    'Positive Score', 'Negative Score', 'Polarity Score', 'Subjectivity Score',
    'Average Sentence Length', 'Percentage of Complex Words', 'Fog Index',
    'Word Count', 'Complex Word Count', 'Syllable Count Per Word',
    'Personal Pronouns', 'Average Word Length'
]

# Update output_df with computed values
for var in variables:
    output_df[var] = analysis_df[var]

# Save to Excel
output_df.to_excel('Output_Data_Updated.xlsx', index=False)